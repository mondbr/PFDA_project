{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://beyondthestates.com/wp-content/uploads/2023/09/download.png\" width=20% height=20%>\n",
    "\n",
    "# Higher Diploma in Data Analytics course from [ATU](https://www.atu.ie/) in Winter 2024/25\n",
    "##  Programming for Data Analytics - project 2024/2025\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"img/Wind_turbines.jpg\" width=60% height=80%>\n",
    "<div style=\"text-align:center; font-size:10px;\"><b>by stockertui @vecteezy.com </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview \n",
    "***\n",
    "\n",
    "In this project I need to demonstrate a data analysis of the wind speed in one of the Ireland's regions.\n",
    "I will analyze 70 years of historical wind speed data from the Roches Point weather station in County Cork, Ireland. The data, sourced from the [Met Ã‰ireann historical dataset](https://www.met.ie/climate/available-data/historical-data), spans from January 1955 to December 2024, providing a comprehensive range for long-term trend analysis.\n",
    "\n",
    "This extensive time range is offering identification of long-term trends, seasonal variations, and other patterns relevant to wind speed and energy production.\n",
    "\n",
    "Roches Point Station is located at the southeastern tip of County Cork, near the entrance to Cork Harbour, a strategic coastal location that often experiences significant wind activity.\n",
    "The station records meteorological data such as wind speed, temperature, and precipitation at hourly intervals, making it suitable for detailed analysis.\n",
    "Its coastal location provides valuable insights into wind patterns influenced by the Atlantic Ocean, making it relevant for assessing wind energy potential in Ireland.\n",
    "\n",
    "Here's what the dataset contains: \n",
    "\n",
    "- date:  -  Date and Time (utc)\n",
    "- rain:  -  Precipitation Amount (mm)\n",
    "- temp:  -  Air Temperature (C)\n",
    "- wetb:  -  Wet Bulb Temperature (C)\n",
    "- dewpt: -  Dew Point Temperature (C)\n",
    "- vappr: -  Vapour Pressure (hPa)\n",
    "- rhum:  -  Relative Humidity (%) \n",
    "- msl:   -  Mean Sea Level Pressure (hPa)\n",
    "- wdsp:  -  Mean Wind Speed (kt)\n",
    "- wddir: -  Predominant Wind Direction (deg)\n",
    "- ind:   -  Indicator\n",
    "\n",
    "To perform the analysis I will need two variables `wdsp` and `wddir`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Project Goal\n",
    "***\n",
    "\n",
    "The primary goal of this project is to analyze wind speed data `wdsp` to assess its potential for wind energy generation and explore historical trends and patterns. By focusing on one location and utilizing this long-term dataset, I will address the following key questions:\n",
    "\n",
    "- How do wind speeds vary seasonally and annually at Roches Point?\n",
    "- Are there noticeable trends in wind speeds over the past 70 years?\n",
    "- What is the potential wind energy output for this location based on historical data?\n",
    "- Are there seasonal or extreme wind events that impact energy reliability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this analysis I will use [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) and its libraries: \n",
    "\n",
    "[Pandas](https://pandas.pydata.org/):\n",
    "- Used to handle and preprocess the dataset (e.g., reading the CSV file, converting dates, grouping data).\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/):\n",
    "\n",
    "- Used for creating plots to visualize trends and regression results.\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/index.html):\n",
    "\n",
    "- Used to build and fit the linear regression model.\n",
    "\n",
    "[NumPy](https://numpy.org/):\n",
    "\n",
    "- Helps with numerical calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required Libraries\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pandas library. Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is the next library I need. It is a fundamental package for scientific computing in Python that provides a multidimensional array object, various derived objects (such as masked arrays and matrices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, Cleaning and Inspecting the Data\n",
    "***\n",
    "The dataset will be loaded into a Python environment using the `pandas` library. After loading, the data will be inspected for its structure, including column names, data types, and missing values etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is saved in .csv file in this repository. After inspecting this data by reviewing the file, I need to ignore first 17 rows as they contain description elements and they are not part of the dataset, therefore they can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>rain</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>temp</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>msl</th>\n",
       "      <th>ind.3</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>ind.4</th>\n",
       "      <th>wddir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-dec-1955 01:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.8</td>\n",
       "      <td>91</td>\n",
       "      <td>1002.1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-dec-1955 02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99</td>\n",
       "      <td>1001.8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-dec-1955 03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>97</td>\n",
       "      <td>1001.7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  ind rain  ind.1  temp  ind.2  wetb dewpt vappr rhum  \\\n",
       "0  01-dec-1955 01:00    0  0.0      0  10.7      0  10.0   9.4  11.8   91   \n",
       "1  01-dec-1955 02:00    0  2.9      0   9.8      0   9.7  10.0  12.0   99   \n",
       "2  01-dec-1955 03:00    0  3.8      0   9.7      0   9.5   9.4  11.7   97   \n",
       "\n",
       "      msl  ind.3 wdsp  ind.4 wddir  \n",
       "0  1002.1      1   16      1   170  \n",
       "1  1001.8      1   11      1   190  \n",
       "2  1001.7      1    9      1   160  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/hly1075.csv', skiprows=17, low_memory=False) # https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date     object\n",
      "ind       int64\n",
      "rain     object\n",
      "ind.1     int64\n",
      "temp     object\n",
      "ind.2     int64\n",
      "wetb     object\n",
      "dewpt    object\n",
      "vappr    object\n",
      "rhum     object\n",
      "msl      object\n",
      "ind.3     int64\n",
      "wdsp     object\n",
      "ind.4     int64\n",
      "wddir    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.dtypes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see both columns `wdsp` and `wddir` are showing as objects which means they might have contains NaN values. Let's check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    ind   rain  ind.1   temp  ind.2   wetb  dewpt  vappr   rhum  \\\n",
      "0       False  False  False  False  False  False  False  False  False  False   \n",
      "1       False  False  False  False  False  False  False  False  False  False   \n",
      "2       False  False  False  False  False  False  False  False  False  False   \n",
      "3       False  False  False  False  False  False  False  False  False  False   \n",
      "4       False  False  False  False  False  False  False  False  False  False   \n",
      "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "585546  False  False  False  False  False  False  False  False  False  False   \n",
      "585547  False  False  False  False  False  False  False  False  False  False   \n",
      "585548  False  False  False  False  False  False  False  False  False  False   \n",
      "585549  False  False  False  False  False  False  False  False  False  False   \n",
      "585550  False  False  False  False  False  False  False  False  False  False   \n",
      "\n",
      "          msl  ind.3   wdsp  ind.4  wddir  \n",
      "0       False  False  False  False  False  \n",
      "1       False  False  False  False  False  \n",
      "2       False  False  False  False  False  \n",
      "3       False  False  False  False  False  \n",
      "4       False  False  False  False  False  \n",
      "...       ...    ...    ...    ...    ...  \n",
      "585546  False  False  False  False  False  \n",
      "585547  False  False  False  False  False  \n",
      "585548  False  False  False  False  False  \n",
      "585549  False  False  False  False  False  \n",
      "585550  False  False  False  False  False  \n",
      "\n",
      "[585551 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "nan_values = df.isna()\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date     0\n",
      "ind      0\n",
      "rain     0\n",
      "ind.1    0\n",
      "temp     0\n",
      "ind.2    0\n",
      "wetb     0\n",
      "dewpt    0\n",
      "vappr    0\n",
      "rhum     0\n",
      "msl      0\n",
      "ind.3    0\n",
      "wdsp     0\n",
      "ind.4    0\n",
      "wddir    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values\n",
    "df.isnull().sum()\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using basic functions `df.isnull()` that shows True/False for missing values and `df.isnull().sum()` that shows total missing values per column, we can see that there is no missing/empty cells in this dataset. That would mean the cells that contain no data are not empty or zeroed, but they might contain a string placeholder for no-data cells. For more information you can look [here.](https://saturncloud.io/blog/how-to-check-if-a-particular-cell-in-pandas-dataframe-is-null/#:~:text=Checking%20for%20Null%20Values%20in%20Pandas%20DataFrame&text=To%20check%20for%20null%20values%20in%20a%20pandas%20DataFrame%2C%20we,cell%20is%20null%20or%20not.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out, I will use the `unique()` function in Pandas. It is used to find all the distinct or unique values in a specific column. After running it, we can quickly understand the variety of values present in data, which in this case is a white space. For more information, you can look at documentation [here](https://www.w3resource.com/numpy/manipulation/unique.php#:~:text=unique()%20function%20is%20used,that%20reconstruct%20the%20input%20array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16' '11' '9' '5' '12' '15' '13' '14' '7' '4' '3' '6' '1' '0' '21' '19'\n",
      " '23' '22' '20' '8' '2' '17' '18' '10' '25' '29' '26' '24' '27' '28' '30'\n",
      " '31' '36' '32' '33' '34' '40' '41' '42' '38' '35' '46' '48' '44' '37'\n",
      " '39' '43' '47' '50' '53' '54' '45' '51' '55' '49' '52' '59' '56' ' ' '62'\n",
      " '60']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values in the Mean Wind Speed column\n",
    "unique_wdsp = df['wdsp'].unique()\n",
    "\n",
    "print(unique_wdsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['170' '190' '160' '140' '330' '340' '350' '320' '360' '10' '0' '230'\n",
      " '210' '200' '220' '250' '260' '280' '300' '240' '270' '310' '150' '180'\n",
      " '290' '40' '20' '50' '70' '60' '80' '110' '130' '100' '90' '120' '30' ' ']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values in the Predominant Wind Direction column\n",
    "unique_wddir = df['wddir'].unique()\n",
    "\n",
    "print(unique_wddir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the range of unique values, in both columns appears to be `' '` value, which is a white space. That is why our data type was shown as object. To be able to use this data for analysing, I need to clean this data. \n",
    "I will create a new dataset `cleandf`. It will replace white spaces with `NaN` and then drop them from my dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf = df[['wddir', 'wdsp']].copy()  # Make a copy of the dataframe\n",
    "\n",
    "# Replace spaces with NaN for both columns\n",
    "cleandf['wdsp'] = cleandf['wdsp'].replace(' ', np.nan)\n",
    "cleandf['wddir'] = cleandf['wddir'].replace(' ', np.nan)\n",
    "\n",
    "# Drop rows with NaN in either 'wdsp' or 'wddir'\n",
    "cleandf.dropna(subset=['wdsp', 'wddir'], inplace=True)\n",
    "\n",
    "# Convert 'wdsp' and 'wddir' to float\n",
    "cleandf['wdsp'] = cleandf['wdsp'].astype(float)\n",
    "cleandf['wddir'] = cleandf['wddir'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        wddir   wdsp\n",
      "0       False  False\n",
      "1       False  False\n",
      "2       False  False\n",
      "3       False  False\n",
      "4       False  False\n",
      "...       ...    ...\n",
      "585546  False  False\n",
      "585547  False  False\n",
      "585548  False  False\n",
      "585549  False  False\n",
      "585550  False  False\n",
      "\n",
      "[584955 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "nan_values = cleandf.isna()\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wddir    0\n",
      "wdsp     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values\n",
    "cleandf.isnull().sum()\n",
    "print(cleandf.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wddir    float64\n",
      "wdsp     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Checking the data type of each column\n",
    "cleandf.dtypes\n",
    "print(cleandf.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
